{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Statistics for Perception Package Projects\n",
    "This example notebook shows how to use datasetinsights to load synthetic datasets generated from the [Perception package](https://github.com/Unity-Technologies/com.unity.perception) and visualize dataset statistics. It includes statistics and visualizations of the outputs built into the Perception package and should give a good idea of how to use datasetinsights to visualize custom annotations and metrics.\n",
    "\n",
    "## Setup dataset\n",
    "If the dataset was generated locally, point `data_root` below to the path of the dataset. The `GUID` folder suffix should be changed accordingly.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/data/zMvA31N\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the metrics needed for dataset statistics.\n",
    "\n",
    "Provide the `run-execution-id` which generated the dataset and a valid `access_token` in the following cell. The `access_token` can be generated using the Unity Simulation [CLI](https://github.com/Unity-Technologies/Unity-Simulation-Docs/blob/master/doc/cli.md#usim-inspect-auth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.io.downloader import UnitySimulationDownloader\n",
    "\n",
    "#run execution id:\n",
    "run_execution_id = \"zMvA31N\"\n",
    "#access_token:\n",
    "access_token = \"uXpdOn9rK8YxOb9sRpAayXCL2oHg_3u14fk_PRlqgsE001f\"\n",
    "#annotation definition id:\n",
    "annotation_definition_id = \"6716c783-1c0e-44ae-b1b5-7f068454b66e\"\n",
    "#unity project id\n",
    "project_id = \"63709827-ed41-46ba-844c-4d4b2eac14fb\"\n",
    "source_uri = f\"usim://{project_id}/{run_execution_id}\"\n",
    "\n",
    "downloader = UnitySimulationDownloader(access_token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the dataset metadata for statistics we first download the relevant files from Unity Simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.download(source_uri=source_uri, output=data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset metadata\n",
    "Once the dataset metadata is downloaded, it can be loaded for statistics using `datasetinsights.data.simulation`. Annotation and metric definitions are loaded into pandas dataframes using `AnnotationDefinitions` and `MetricDefinitions` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.datasets.unity_perception import AnnotationDefinitions, MetricDefinitions\n",
    "ann_def = AnnotationDefinitions(data_root)\n",
    "ann_def.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_def = MetricDefinitions(data_root)\n",
    "metric_def.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Statistics\n",
    "The following tables and charts are supplied by `datasetinsights.data.datasets.statistics.RenderedObjectInfo` on datasets that include the \"rendered object info\" metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.stats.statistics import RenderedObjectInfo\n",
    "import datasetinsights.datasets.unity_perception.metrics as metrics\n",
    "from datasetinsights.datasets.unity_perception.exceptions import DefinitionIDError\n",
    "from datasetinsights.stats import bar_plot, histogram_plot, rotation_plot\n",
    "\n",
    "max_samples = 10000          # maximum number of samples points used in histogram plots\n",
    "\n",
    "rendered_object_info_definition_id = \"5ba92024-b3b7-41a7-9d3f-c03a6a8ddd01\"\n",
    "roinfo = None\n",
    "try:\n",
    "    roinfo = RenderedObjectInfo(data_root=data_root, def_id=rendered_object_info_definition_id)\n",
    "except DefinitionIDError:\n",
    "    print(\"No RenderedObjectInfo in this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    print(roinfo.num_captures())\n",
    "    roinfo.raw_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    total_count = roinfo.total_counts()\n",
    "    display(total_count)\n",
    "    \n",
    "    display(bar_plot(\n",
    "        total_count, \n",
    "        x=\"label_id\", \n",
    "        y=\"count\", \n",
    "        x_title=\"Label Name\",\n",
    "        y_title=\"Count\",\n",
    "        title=\"Total Object Count in Dataset\",\n",
    "        hover_name=\"label_name\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Capture Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    per_capture_count = roinfo.per_capture_counts()\n",
    "    display(per_capture_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        per_capture_count, \n",
    "        x=\"count\",  \n",
    "        x_title=\"Object Counts Per Capture\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Object Counts Per Capture\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Visible Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        roinfo.raw_table, \n",
    "        x=\"visible_pixels\",  \n",
    "        x_title=\"Visible Pixels Per Object\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Visible Pixels Per Object\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Visualization\n",
    "In the following sections we show how to load annotations from the Captures object and visualize them. Similar code can be used to consume annotations for model training or visualize and train on custom annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the images, captures and annotations in the dataset. Make sure you have enough disk space to store all files. For example, a dataset with 100K captures requires roughly 300GiB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "downloader.download(source_uri=source_uri, output=data_root, include_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.datasets.unity_perception.captures import Captures\n",
    "cap = Captures(data_root)\n",
    "cap.captures.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Boxes\n",
    "In this section we render 2d bounding boxes on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def cleanup(catalog):\n",
    "    catalog = remove_captures_with_missing_files(data_root, catalog)\n",
    "    catalog = remove_captures_without_bboxes(catalog)\n",
    "    return catalog\n",
    "\n",
    "def remove_captures_without_bboxes(catalog):\n",
    "    keep_mask = catalog[\"annotation.values\"].apply(len) > 0\n",
    "    return catalog[keep_mask]\n",
    "\n",
    "def remove_captures_with_missing_files(root, catalog):\n",
    "    def exists(capture_file):\n",
    "        path = Path(root) / capture_file\n",
    "        return path.exists()\n",
    "    keep_mask = catalog.filename.apply(exists)\n",
    "    return catalog[keep_mask]\n",
    "\n",
    "def capture_df(def_id):\n",
    "    captures = Captures(data_root)\n",
    "    catalog = captures.filter(bounding_box_definition_id)\n",
    "    catalog=cleanup(catalog)\n",
    "    return catalog\n",
    "\n",
    "def label_mappings_dict(def_id):\n",
    "    annotation_def = AnnotationDefinitions(data_root)\n",
    "    init_definition = annotation_def.get_definition(bounding_box_definition_id)\n",
    "    label_mappings = {\n",
    "        m[\"label_id\"]: m[\"label_name\"] for m in init_definition[\"spec\"]\n",
    "    }\n",
    "    return label_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from PIL import Image\n",
    "\n",
    "from datasetinsights.stats.visualization.plots import plot_bboxes\n",
    "from datasetinsights.datasets.synthetic import read_bounding_box_2d\n",
    "\n",
    "bounding_box_definition_id = \"c31620e3-55ff-4af6-ae86-884aa0daa9b2\"\n",
    "\n",
    "try:\n",
    "    catalog= capture_df(bounding_box_definition_id)\n",
    "    label_mappings=label_mappings_dict(bounding_box_definition_id)\n",
    "except DefinitionIDError:\n",
    "    print(\"No bounding boxes found\")\n",
    "    \n",
    "def draw_bounding_boxes(index):\n",
    "    cap = catalog.iloc[index]\n",
    "    capture_file = cap.filename\n",
    "    ann = cap[\"annotation.values\"]\n",
    "    capture = Image.open(os.path.join(data_root, capture_file))\n",
    "    image = capture.convert(\"RGB\")  # Remove alpha channel\n",
    "    bboxes = read_bounding_box_2d(ann, label_mappings)\n",
    "    return plot_bboxes(image, bboxes, label_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "# pick an index and visualize\n",
    "interact(draw_bounding_boxes, index=list(range(len(catalog))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thea.datasets import Dataset\n",
    "data_path = \"/data/groceries\"\n",
    "raw_real_images = Dataset.create(\"GroceriesReal\", data_path=data_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_bounding_boxes(index):\n",
    "    cap = catalog.iloc[index]\n",
    "    capture_file = cap.filename\n",
    "    ann = cap[\"annotation.values\"]\n",
    "    capture = Image.open(os.path.join(data_root, capture_file))\n",
    "    image = capture.convert(\"RGB\")  # Remove alpha channel\n",
    "    bboxes = read_bounding_box_2d(ann, label_mappings)\n",
    "    return plot_bboxes(image, bboxes, label_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_downloaded_images(catalog):\n",
    "    images = []\n",
    "    for index in range(len(catalog[:1000])):\n",
    "        cap = catalog.iloc[index]\n",
    "        capture_file = cap.filename\n",
    "        capture = Image.open(os.path.join(data_root, capture_file))\n",
    "        image = capture.convert(\"L\")\n",
    "        npix = image.size[1]\n",
    "        image = image.resize((npix, npix))\n",
    "        image = np.asarray(image)\n",
    "        images.append(image)\n",
    "    images = np.dstack(images)\n",
    "    images = np.rollaxis(images, -1)\n",
    "    return images\n",
    "\n",
    "def get_real_images_numpy(real_images):\n",
    "    images = []\n",
    "    for i in range(len(real_images)):\n",
    "        image = real_images[i][0]\n",
    "        image = image.convert(\"L\")\n",
    "        npix = image.size[1]\n",
    "        image = image.resize((npix, npix))\n",
    "        image = np.asarray(image)\n",
    "        images.append(image)\n",
    "    images = np.dstack(images)\n",
    "    images = np.rollaxis(images, -1)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_power_spectrum(image):\n",
    "    ''' Get power spectrum for one image\n",
    "    \n",
    "    Args:\n",
    "        image: 2d numpy array (shape H, W)\n",
    "    \n",
    "    Returns:\n",
    "        fourier_amplitudes: 2d numpy array (shape H, W)\n",
    "        fourier_image: 2d numpy array (shape H, W)\n",
    "        Abins: 1d numpy array, azimuthal averaged power spectrum density (shape: (H * W / 2 + 1,))\n",
    "    '''\n",
    "    npix = image.shape[0]\n",
    "    # Fourier transform of two dimensional image data array. (960, 960)\n",
    "    fourier_image = np.fft.fftn(image) # fft2\n",
    "    # we only require the square of the amplitudes to compute the variances. (960, 960)\n",
    "    fourier_amplitudes = np.abs(fourier_image)**2 \n",
    "    fourier_image = np.fft.fftshift(fourier_image)\n",
    "    \n",
    "    # To bin the results found above in k space, we need to know what the layout of\n",
    "    # the return value of numpy.fft.fftn is.\n",
    "    # # This will automatically return a one dimensional array containing the wave vectors \n",
    "    # for the numpy.fft.fftn call, in the correct order. (960,)\n",
    "    kfreq = np.fft.fftfreq(npix) * npix\n",
    "    # Convert this to a two dimensional array matching the layout of the two dimensional Fourier image. [(960, 960), (960, 960)]\n",
    "    kfreq2D = np.meshgrid(kfreq, kfreq)\n",
    "    # we are not really interested in the actual wave vectors, but rather in their norm\n",
    "    knrm = np.sqrt(kfreq2D[0]**2 + kfreq2D[1]**2)\n",
    "    # we no longer need the wave vector norms or Fourier image to be laid out as a two dimensional array,\n",
    "    # so we will flatten them\n",
    "    knrm = knrm.flatten()\n",
    "    fourier_amplitudes = fourier_amplitudes.flatten()\n",
    "    # To bin the amplitudes in k space, we need to set up wave number bins. We will create integer k value bins\n",
    "    kbins = np.arange(0.5, npix//2+1, 1.)\n",
    "    # The kbin array will contain the start and end points of all bins; the corresponding k values\n",
    "    # are the midpoints of these bins\n",
    "    kvals = 0.5 * (kbins[1:] + kbins[:-1])\n",
    "    # Compute the average Fourier amplitude (squared) in each bin\n",
    "    Abins, _, _ = stats.binned_statistic(knrm, fourier_amplitudes,\n",
    "                                         statistic = \"mean\",\n",
    "                                         bins = kbins)\n",
    "    # We want the total variance within each bin. Right now, we only have the average power.\n",
    "    # To get the total power, we need to multiply with the volume in each bin \n",
    "    Abins *= np.pi * (kbins[1:]**2 - kbins[:-1]**2)\n",
    "    return fourier_image, kvals, Abins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The analysis will only work for a square image\n",
    "def get_average_fourier_amplitude(images):\n",
    "    ''' Average fourier amplitude across images\n",
    "    \n",
    "    Args:\n",
    "        images: 3d numpy array (num of images, H, W)\n",
    "    \n",
    "    Returns:\n",
    "        kvals:\n",
    "        average_amplitude: \n",
    "    '''\n",
    "    npix = images.shape[1]\n",
    "    average_amplitude = np.zeros(npix // 2)\n",
    "    total_fourier_image = np.zeros((npix, npix), dtype='complex128')\n",
    "    # image size 960 * 960\n",
    "    for image in images:   \n",
    "        fourier_image, kvals, Abins = get_image_power_spectrum(image)\n",
    "        # Sum all the amplitude at each bin\n",
    "        average_amplitude += Abins\n",
    "        # shum all image\n",
    "        total_fourier_image += fourier_image\n",
    "    n = images.shape[0]\n",
    "    # take average\n",
    "    average_amplitude /= n\n",
    "    averaged_fourier_image = total_fourier_image / n\n",
    "    return kvals, averaged_fourier_image, average_amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_fourier_image(data1=None, title1 = \"\", data2=None, title2=\"\"):\n",
    "    ''' Plot averaged fourier images of two datasets\n",
    "\n",
    "    Args:\n",
    "        data1: numpy.ndarray (shape H, W)\n",
    "        title1: string, title of the first plot\n",
    "        data2: numpy.ndarray (shape H, W)\n",
    "        title2: string, title of the second plot\n",
    "\n",
    "    Returns:\n",
    "    '''\n",
    "    image1 = np.log(np.abs(data1) ** 2)\n",
    "    image2 = np.log(np.abs(data2) ** 2)\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.imshow(image1)\n",
    "    ax.set_title(title1)\n",
    "    ax1 = fig.add_subplot(1,2,2)\n",
    "    ax1.imshow(image2)\n",
    "    ax1.set_title(title2)\n",
    "\n",
    "def plot_images(image1=None, title1=\"\", image2=None, title2=\"\"):\n",
    "    ''' Plot averaged fourier images of two datasets\n",
    "    \n",
    "    Args:\n",
    "        data1: PIL image (shape H, W)\n",
    "        title1: string, title of the first plot\n",
    "        data2: PIL image (shape H, W)\n",
    "        title2: string, title of the second plot\n",
    "    \n",
    "    Returns:\n",
    "    '''\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    ax = fig.add_subplot(1,2,1)\n",
    "    ax.imshow(image1)\n",
    "    plt.axis('off')\n",
    "    ax.set_title(title1)\n",
    "    ax1 = fig.add_subplot(1,2,2)\n",
    "    ax1.imshow(image2)\n",
    "    ax1.set_title(title2)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_images = get_downloaded_images(catalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_images = get_real_images_numpy(raw_real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvals_synth, averaged_fourier_image_synth, average_amplitude_synth = get_average_fourier_amplitude(synth_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvals_real, averaged_fourier_image_real, average_amplitude_real = get_average_fourier_amplitude(real_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('average_amplitude_real.txt', 'w') as filehandle:\n",
    "    for listitem in average_amplitude_real:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kvals_real.txt', 'w') as filehandle:\n",
    "    for listitem in kvals_real:\n",
    "        filehandle.write('%s\\n' % listitem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fourier_image(data1=averaged_fourier_image_synth, title1 = \"Synthetic data\", data2=averaged_fourier_image_real, title2=\"Real-world data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals_synth,\n",
    "    y=average_amplitude_synth,\n",
    "    mode='lines+markers',\n",
    "    line=dict(dash='dot'),\n",
    "    name=\"synth\"\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals_real,\n",
    "    y=average_amplitude_real,\n",
    "    mode='lines+markers',\n",
    "    line=dict(dash='dot'),\n",
    "    name=\"real-world\"\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Azimuthal averaged power spectrum density\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(k)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_image(image):\n",
    "    image = image.convert(\"L\")\n",
    "    npix = image.size[1]\n",
    "    image = image.resize((npix, npix))\n",
    "    image = np.asarray(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 100\n",
    "original_image = raw_real_images[index][0]\n",
    "original_numpy_image = convert_numpy_image(original_image)\n",
    "\n",
    "fourier_image, kvals, Abins = get_image_power_spectrum(original_numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(np.abs(fourier_image) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(original_image)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals,\n",
    "    y=Abins,\n",
    "    mode='lines+markers',\n",
    "    name='original'\n",
    "))\n",
    "  \n",
    "fig.update_layout(\n",
    "    title=\"Azimuthal averaged power spectrum density\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(k)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blur effect (Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFilter\n",
    "blur_image = original_image.filter(ImageFilter.GaussianBlur(5))\n",
    "blur_numpy_image = convert_numpy_image(blur_image)\n",
    "\n",
    "fourier_image_blur, kvals_blur, Abins_blur = get_image_power_spectrum(blur_numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(image1=original_image, title1='Original', image2=blur_image, title2='Blur')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fourier_image(data1=fourier_image, title1 = \"Original\", data2=fourier_image_blur, title2=\"Blur\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals_blur,\n",
    "    y=Abins_blur,\n",
    "    mode='lines+markers',\n",
    "    name='blur'\n",
    "))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals,\n",
    "    y=Abins,\n",
    "    mode='lines+markers',\n",
    "    name='origianl'\n",
    "))\n",
    "  \n",
    "fig.update_layout(\n",
    "    title=\"Azimuthal averaged power spectrum density\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(k)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageEnhance\n",
    "contrast_effect = \"High contrast\"\n",
    "contrast_image = ImageEnhance.Contrast(original_image).enhance(1.5)\n",
    "contrast_numpy_image = convert_numpy_image(contrast_image)\n",
    "\n",
    "fourier_image_contrast, kvals_contrast, Abins_contrast = get_image_power_spectrum(contrast_numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(image1=original_image, title1='Original', image2=contrast_image, title2=contrast_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fourier_image(data1=fourier_image, title1 = \"Original\", data2=fourier_image_contrast, title2=contrast_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals_contrast,\n",
    "    y=Abins_contrast,\n",
    "    mode='lines+markers',\n",
    "    name=contrast_effect\n",
    "))\n",
    "  \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals,\n",
    "    y=Abins,\n",
    "    mode='lines+markers',\n",
    "    name='origianl'\n",
    "))\n",
    "  \n",
    "fig.update_layout(\n",
    "    title=\"Azimuthal averaged power spectrum density\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(k)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saturation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageEnhance\n",
    "saturation_effect = \"High saturation\"\n",
    "saturation_image = ImageEnhance.Color(original_image).enhance(1.5)\n",
    "saturation_numpy_image = convert_numpy_image(saturation_image)\n",
    "\n",
    "fourier_image_saturation, kvals_saturation, Abins_saturation = get_image_power_spectrum(saturation_numpy_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(image1=original_image, title1='Original', image2=saturation_image, title2=saturation_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_fourier_image(data1=fourier_image, title1 = \"Original\", data2=fourier_image_saturation, title2=saturation_effect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals_saturation,\n",
    "    y=Abins_saturation,\n",
    "    mode='lines+markers',\n",
    "    name=saturation_effect\n",
    "))\n",
    "  \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=kvals,\n",
    "    y=Abins,\n",
    "    mode='lines+markers',\n",
    "    name='origianl'\n",
    "))\n",
    "  \n",
    "fig.update_layout(\n",
    "    title=\"Azimuthal averaged power spectrum density\",\n",
    "    xaxis_type=\"log\",\n",
    "    yaxis_type=\"log\",\n",
    "    xaxis_title=\"k\",\n",
    "    yaxis_title=\"P(k)\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3D Ground Truth Bounding Boxes\n",
    "In this section we render 3d ground truth bounding boxes on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "from datasetinsights.stats.visualization.plots import plot_bboxes3d\n",
    "from datasetinsights.datasets.synthetic import read_bounding_box_3d\n",
    "\n",
    "bounding_box_3d_defintion_id = \"0bfbe00d-00fa-4555-88d1-471b58449f5c\"\n",
    "def draw_bounding_boxes3d(index):\n",
    "    filename = os.path.join(data_root, box_captures.loc[index, \"filename\"])\n",
    "    annotations = box_captures.loc[index, \"annotation.values\"]\n",
    "    sensor = box_captures.loc[index, \"sensor\"]\n",
    "\n",
    "    if 'camera_intrinsic' in sensor:\n",
    "        projection = np.array(sensor[\"camera_intrinsic\"])\n",
    "    else:\n",
    "        projection = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "\n",
    "    image = Image.open(filename)\n",
    "    boxes = read_bounding_box_3d(annotations)\n",
    "    img_with_boxes = plot_bboxes3d(image, boxes, projection)\n",
    "    img_with_boxes.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_boxes)\n",
    "\n",
    "try:\n",
    "    box_captures = cap.filter(def_id=bounding_box_3d_defintion_id)\n",
    "    interact(draw_bounding_boxes3d, index=(0, box_captures.shape[0]))\n",
    "except DefinitionIDError:\n",
    "    print(\"No bounding boxes found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "In this section we render the semantic segmentation images on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_with_segmentation(index, opacity):\n",
    "    filename = os.path.join(data_root, seg_captures.loc[index, \"filename\"])\n",
    "    seg_filename = os.path.join(data_root, seg_captures.loc[index, \"annotation.filename\"])\n",
    "    \n",
    "    image = Image.open(filename)\n",
    "    seg = Image.open(seg_filename)\n",
    "    img_with_seg = Image.blend(image, seg, opacity)\n",
    "    img_with_seg.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_seg)\n",
    "    \n",
    "try:\n",
    "    semantic_segmentation_definition_id = \"12f94d8d-5425-4deb-9b21-5e53ad957d66\"\n",
    "    seg_captures = cap.filter(def_id=semantic_segmentation_definition_id)\n",
    "    interact(draw_with_segmentation, index=(0, seg_captures.shape[0]), opacity=(0.0, 1.0))\n",
    "except DefinitionIDError:\n",
    "    print(\"No semantic segmentation images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Segmentation\n",
    "In this section we render the instance segmentation images on top of the captured images. Image IDs are mapped to an RGBA color value, below the image we include a preview of the mapping between colors and IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instance_sorter(instance):\n",
    "    return instance[\"instance_id\"]\n",
    "\n",
    "def draw_with_instance_segmentation(index, opacity):\n",
    "    filename = os.path.join(data_root, inst_caps.loc[index, \"filename\"])\n",
    "    seg_filename = os.path.join(data_root, inst_caps.loc[index, \"annotation.filename\"])\n",
    "\n",
    "    image = Image.open(filename)\n",
    "    seg = Image.open(seg_filename)\n",
    "    img_with_seg = Image.blend(image, seg, opacity)\n",
    "    img_with_seg.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_seg)\n",
    "\n",
    "    anns = inst_caps.loc[index, \"annotation.values\"].copy()\n",
    "    anns.sort(key=instance_sorter)\n",
    "\n",
    "    count = min(5, len(anns))\n",
    "    print(\"First {} ID entries:\".format(count))\n",
    "\n",
    "    for i in range(count):\n",
    "        color = anns[i].get(\"color\")\n",
    "        print (\"{} => Color({:>3}, {:>3}, {:>3})\".format(anns[i].get(\"instance_id\"), color.get(\"r\"), color.get(\"g\"), color.get(\"b\")))\n",
    "\n",
    "try:\n",
    "    inst_seg_def_id = \"1ccebeb4-5886-41ff-8fe0-f911fa8cbcdf\"\n",
    "    inst_caps = cap.filter(def_id=inst_seg_def_id)\n",
    "    interact(draw_with_instance_segmentation, index=(0, inst_caps.shape[0]), opacity=(0.0, 1.0))\n",
    "except DefinitionIDError:\n",
    "    print(\"No instance segmentation images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoints\n",
    "In this section we render the keypoint labeled data for the captured frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from datasetinsights.stats.visualization.plots import plot_keypoints\n",
    "\n",
    "\n",
    "def draw_human_pose(index):\n",
    "    filename = os.path.join(data_root, keypoint_caps.loc[index, \"filename\"])\n",
    "    annotations = keypoint_caps.loc[index, \"annotation.values\"]\n",
    "    templates = ann_def.get_definition(keypoint_def_id)['spec']\n",
    "    img = Image.open(filename)\n",
    "    img_with_pose = plot_keypoints(img, annotations, templates)\n",
    "    img_with_pose.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_pose)\n",
    "\n",
    "\n",
    "try:\n",
    "    keypoint_def_id = \"8b3ef246-daa7-4dd5-a0e8-a943f6e7f8c2\"\n",
    "    keypoint_caps = cap.filter(def_id=keypoint_def_id)\n",
    "    interact(draw_human_pose, index=(0, keypoint_caps.shape[0] - 1))\n",
    "except DefinitionIDError:\n",
    "    print(\"No keypoint data found\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
